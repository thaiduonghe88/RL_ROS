{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Tests and Functionality Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find the different functionality given by the **OpenAI_ROS** package, shown through a series of demos. Please enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Easy Plug and play environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the power of openai, we take it a step further creating **robot-env** and **task-env** for further modularity. Here you ahe a demo with a **Cartpole3D Simulation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Simulations DropDown Menu** select: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cartpole_description main.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get simething like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cartpole.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on the 2D demo simulation created by the people in OpenAI: \n",
    "* Wiki for Cartpole-v0: https://github.com/openai/gym/wiki/CartPole-v0\n",
    "* Cartpole Env definition:\n",
    "https://github.com/openai/gym/blob/5404b39d06f72012f562ec41f60734bd4b5ceb4b/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can adapt any algorithm for Reinforcement learning to OpenAI_ROS, as we demonstrate in the following example. We took several of the best algorithms posted by the users to solve the 2D cartpole system: **Algorithms for Solution Made by users**\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* n1try: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "* mbalunovic: https://gym.openai.com/evaluations/eval_OeUSZwUcR2qSAqMmOE1UIw/\n",
    "* ruippeixotog: https://gym.openai.com/evaluations/eval_aCiCDmwhTCytFuxMpKoyvQ/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we adapted it for ROS and to use our Cartpole3D environment. For that we just had to do mainly 2 things:\n",
    "* Load the learning parameters from a yaml file insted of loding them from GLOBAL variables. This allows us to change the values easily and fast. So we have to create a yaml file for each algorithm that will be loaded before the algorithm is launched. \n",
    "* We have change the **OpenAI** environment from the **2DCartpole** to the one loaded through **OpenAI_ROS** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt and create YAML files for the learning Parameters to be loaded through ROSPARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of of the **yaml** file for the **n1try**. Its divided in two main parts:\n",
    "* Parameters for Reinforcement Learning and algorithm: Values to  needed by your learning algorithm, in this case would be n1try, but could be any.\n",
    "* Parameters for the RobotEnvironment and TaskEnvironment: These varaiables are used to tune the task and simulation. These variables have already been set up for you to be optimum, and if you are asking where you would get them from the first place, you can just copy paste the ones in the Example Git for OpenAI_ROS, where you can find examples for all the supported simulations and tasks.\n",
    "https://bitbucket.org/theconstructcore/openai_examples_projects/src/master/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_n1try_params.yaml** found in the package **cartpole_tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    \n",
    "    alpha: 0.01 # Learning Rate\n",
    "    alpha_decay: 0.01\n",
    "    gamma: 1.0 # future rewards value 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.01 # minimum value that epsilon can have\n",
    "    batch_size: 64 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    n_win_ticks: 250 # If the mean of rewards is bigger than this and have passed min_episodes, the task is considered finished\n",
    "    min_episodes: 100\n",
    "    #max_env_steps: None\n",
    "    monitor: True\n",
    "    quiet: False\n",
    "    \n",
    "\n",
    "    # Cartpole3D environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    n_observations: 4 # Number of lasers to consider in the observations\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to change the original python script to accomodate for these changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n1tryx.py** **original main script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspired by https://keon.io/deep-q-learning/\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=4, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole3D_n1try_algorithm_with_rosparams.py** **adapted script main script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "\n",
    "# Inspired by https://keon.io/deep-q-learning/\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "class DQNRobotSolver():\n",
    "    def __init__(self, environment_name, n_observations, n_actions, n_episodes=1000, n_win_ticks=195, min_episodes= 100, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(environment_name)\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        \n",
    "        self.input_dim = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.min_episodes = min_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(24, input_dim=self.input_dim, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.input_dim])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        rate = rospy.Rate(30)\n",
    "        \n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            \n",
    "            init_state = self.env.reset()\n",
    "            \n",
    "            state = self.preprocess_state(init_state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # openai_ros doesnt support render for the moment\n",
    "                #self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= min_episodes:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials'.format(e, e - min_episodes))\n",
    "                return e - min_episodes\n",
    "            if e % 1 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last {} episodes was {} ticks.'.format(e, min_episodes ,mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "            \n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes'.format(e))\n",
    "        return e\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('cartpole_n1try_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    environment_name = 'CartPoleStayUp-v0'\n",
    "    \n",
    "    n_observations = rospy.get_param('/cartpole_v0/n_observations')\n",
    "    n_actions = rospy.get_param('/cartpole_v0/n_actions')\n",
    "    \n",
    "    n_episodes = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "    n_win_ticks = rospy.get_param('/cartpole_v0/n_win_ticks')\n",
    "    min_episodes = rospy.get_param('/cartpole_v0/min_episodes')\n",
    "    max_env_steps = None\n",
    "    gamma =  rospy.get_param('/cartpole_v0/gamma')\n",
    "    epsilon = rospy.get_param('/cartpole_v0/epsilon')\n",
    "    epsilon_min = rospy.get_param('/cartpole_v0/epsilon_min')\n",
    "    epsilon_log_decay = rospy.get_param('/cartpole_v0/epsilon_decay')\n",
    "    alpha = rospy.get_param('/cartpole_v0/alpha')\n",
    "    alpha_decay = rospy.get_param('/cartpole_v0/alpha_decay')\n",
    "    batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "    monitor = rospy.get_param('/cartpole_v0/monitor')\n",
    "    quiet = rospy.get_param('/cartpole_v0/quiet')\n",
    "    \n",
    "    \n",
    "    agent = DQNRobotSolver(     environment_name,\n",
    "                                n_observations,\n",
    "                                n_actions,\n",
    "                                n_episodes,\n",
    "                                n_win_ticks,\n",
    "                                min_episodes,\n",
    "                                max_env_steps,\n",
    "                                gamma,\n",
    "                                epsilon,\n",
    "                                epsilon_min,\n",
    "                                epsilon_log_decay,\n",
    "                                alpha,\n",
    "                                alpha_decay,\n",
    "                                batch_size,\n",
    "                                monitor,\n",
    "                                quiet)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the main differences are the **rosparam gets** and the **rosnode init** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospy.init_node('cartpole_n1try_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "environment_name = 'CartPoleStayUp-v0'\n",
    "\n",
    "n_observations = rospy.get_param('/cartpole_v0/n_observations')\n",
    "n_actions = rospy.get_param('/cartpole_v0/n_actions')\n",
    "\n",
    "n_episodes = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "n_win_ticks = rospy.get_param('/cartpole_v0/n_win_ticks')\n",
    "min_episodes = rospy.get_param('/cartpole_v0/min_episodes')\n",
    "max_env_steps = None\n",
    "gamma =  rospy.get_param('/cartpole_v0/gamma')\n",
    "epsilon = rospy.get_param('/cartpole_v0/epsilon')\n",
    "epsilon_min = rospy.get_param('/cartpole_v0/epsilon_min')\n",
    "epsilon_log_decay = rospy.get_param('/cartpole_v0/epsilon_decay')\n",
    "alpha = rospy.get_param('/cartpole_v0/alpha')\n",
    "alpha_decay = rospy.get_param('/cartpole_v0/alpha_decay')\n",
    "batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "monitor = rospy.get_param('/cartpole_v0/monitor')\n",
    "quiet = rospy.get_param('/cartpole_v0/quiet')\n",
    "\n",
    "\n",
    "agent = DQNRobotSolver(     environment_name,\n",
    "                            n_observations,\n",
    "                            n_actions,\n",
    "                            n_episodes,\n",
    "                            n_win_ticks,\n",
    "                            min_episodes,\n",
    "                            max_env_steps,\n",
    "                            gamma,\n",
    "                            epsilon,\n",
    "                            epsilon_min,\n",
    "                            epsilon_log_decay,\n",
    "                            alpha,\n",
    "                            alpha_decay,\n",
    "                            batch_size,\n",
    "                            monitor,\n",
    "                            quiet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also one element that has to be removed is the **render method**. Just because in Gazebo Render makes no sense, unless is for recording purposes, that will be supported in OpenAI_ROS RobotEnvironments in the future updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# openai_ros doesnt support render for the moment\n",
    "self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you have to create a **launch file ** thats loads those paramateres from the **YAML** file and starts up your script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_n1try.launch**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find cartpole_tests)/config/cartpole_n1try_params.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"cartpole_tests\" name=\"cartpole3D_mbalunovic_algorithm\" type=\"cartpole3D_n1try_algorithm_with_rosparams.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the algorithm to work with OpenAI_ROS Task-environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the easies part. You just have to change the name of the **OpenAI Environment** to load, and in this case import the **openai_ros** module. And thats it you change the 2D cartpole to the 3D gazebo realistic cartpole. Thats **SIMPLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This at the top in the import section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To a different Task and its loaded through a variable of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "environment_name = 'CartPoleStayUp-v0'\n",
    "\n",
    "agent = DQNRobotSolver(     environment_name,\n",
    "                       ...\n",
    "\n",
    "self.env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it, you can now launch it with your original algorithm untouched pretty much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have a video and the results of executing this algorithm in the 2D default environment and the 3D Gazebo environment using the **same** algorithm. The 2D script is the same shown in the original but adapted to publish the rewards as the 3D Cartpole Envirnment does by default due to **OpenAI_ROS** structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/cartpole_n1try_2D_demo_ROSCON-2018-09-20_14.58.44.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/cartpole_n1ty_demo_ROSCON-2018-09-20_08.22.31.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have here the results of the rward plot through the 1000 episodes done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n1try Cartpole-v0 ( Cartpole 2D default environment)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole2D_n1try.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n1try CartPoleStayUp-v0 ( Cartpole 3D Gazebo environment)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole_n1ty_standard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are tests made with 1000 epsidoes maximum and it will stop the learning if:\n",
    "* We depleat our episodes ( more than 1000 )\n",
    "* We Get a Mean Episode Reward bigger than **n_win_ticks** and we have executed more than **min_episodes** number of episodes. In this case of n_win_ticks = 250  and min_episodes = 100.\n",
    "This is the reason why the 3D version stoped before the 1000 episodes had ended. Because the mean value of the episode rewards was bigger then 250. While the 2D had values not exceeding 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to execute these demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just launch the following:\n",
    "* 2DCartpole_n1try:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roslaunch cartpole_tests start_training_n1try_2D.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3DCartpole_n1try ( first launch the 3D simulation explained how above ):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roslaunch cartpole_tests start_training_n1try.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- How to change Algorithm for the same TaskEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to change your algorithm for the same TaskEnvironment, like the **CartPoleStayUp-v0**?\n",
    "Well, with **OpenAI_ROS** is really fast. You just have to change the following:\n",
    "* Change the ROSParam YAML file loaded in the Launch file\n",
    "* Change the algorithm python script to be launched in the launch file to your new one.\n",
    "* Inside your new algorithm main script, place the same **TaskEnvironment** you had in the previous algorithm script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example lets say you need to change from the **n1try** to the **mbalunovic** or the **ruippeixotog**.\n",
    "* You will have to create **new YAML files** with the ReinforcementAlgorithm paramters you need, but with the same **simulation parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_mbalunovic_params.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    state_size: 4 # number of elements in the state array from observations.\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    learning_rate: 0.001\n",
    "    #num_epochs: 50\n",
    "    replay_memory_size: 1000\n",
    "    target_update_freq: 100\n",
    "    initial_random_action: 1\n",
    "    gamma: 0.99 # future rewards value, 0 none 1 a lot\n",
    "    epsilon_decay: 0.99 # how we reduse the exploration\n",
    "    done_episode_reward: -200 # Reward given when episode finishes before interations depleated.\n",
    "    batch_size: 32 # maximum size of the batches sampled from memory\n",
    "    max_iterations: 1000 # maximum iterations that an episode can have\n",
    "    episodes_training: 1000\n",
    "\n",
    "    # Environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_ruippeixotog_params.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    state_size: 4 # number of elements in the state array from observations.\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    gamma: 0.95 # future rewards value, 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.1 # minimum value that epsilon can have\n",
    "    batch_size: 32 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    episodes_running: 500\n",
    "\n",
    "    #environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create new launch files to load those **yaml** files and also execute the algorithms main learning scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_mbalunovic.launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find cartpole_tests)/config/cartpole_mbalunovic_params.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"cartpole_tests\" name=\"cartpole3D_mbalunovic_algorithm\" type=\"cartpole3D_mbalunovic_algorithm_with_rosparams.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_ruippeixotog.launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find cartpole_tests)/config/cartpole_ruippeixotog_params.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"cartpole_tests\" name=\"cartpole3D_ruippeixotog_algorithm\" type=\"cartpole3D_ruippeixotog_algorithm.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change the Scripts to use the correct **TaskEnvironment**. We suppose that you have changed them to use ROSPARAMS, explained how to in previous section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mbalunovic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbalunoviing Original script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from gym import wrappers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "ACTIONS_DIM = 2\n",
    "OBSERVATIONS_DIM = 4\n",
    "MAX_ITERATIONS = 10**6\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "GAMMA = 0.99\n",
    "REPLAY_MEMORY_SIZE = 1000\n",
    "NUM_EPISODES = 10000\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "MINIBATCH_SIZE = 32\n",
    "\n",
    "RANDOM_ACTION_DECAY = 0.99\n",
    "INITIAL_RANDOM_ACTION = 1\n",
    "\n",
    "class ReplayBuffer():\n",
    "\n",
    "  def __init__(self, max_size):\n",
    "    self.max_size = max_size\n",
    "    self.transitions = deque()\n",
    "\n",
    "  def add(self, observation, action, reward, observation2):\n",
    "    if len(self.transitions) > self.max_size:\n",
    "      self.transitions.popleft()\n",
    "    self.transitions.append((observation, action, reward, observation2))\n",
    "\n",
    "  def sample(self, count):\n",
    "    return random.sample(self.transitions, count)\n",
    "\n",
    "  def size(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "def get_q(model, observation):\n",
    "  np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])\n",
    "  return model.predict(np_obs)\n",
    "\n",
    "def train(model, observations, targets):\n",
    "  # for i, observation in enumerate(observations):\n",
    "  #   np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])\n",
    "  #   print \"t: {}, p: {}\".format(model.predict(np_obs),targets[i])\n",
    "  # exit(0)\n",
    "\n",
    "  np_obs = np.reshape(observations, [-1, OBSERVATIONS_DIM])\n",
    "  np_targets = np.reshape(targets, [-1, ACTIONS_DIM])\n",
    "\n",
    "  model.fit(np_obs, np_targets, epochs=1, verbose=0)\n",
    "\n",
    "def predict(model, observation):\n",
    "  np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])\n",
    "  return model.predict(np_obs)\n",
    "\n",
    "def get_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(16, input_shape=(OBSERVATIONS_DIM, ), activation='relu'))\n",
    "  model.add(Dense(16, input_shape=(OBSERVATIONS_DIM,), activation='relu'))\n",
    "  model.add(Dense(2, activation='linear'))\n",
    "\n",
    "  model.compile(\n",
    "    optimizer=Adam(lr=LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=[],\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "def update_action(action_model, target_model, sample_transitions):\n",
    "  random.shuffle(sample_transitions)\n",
    "  batch_observations = []\n",
    "  batch_targets = []\n",
    "\n",
    "  for sample_transition in sample_transitions:\n",
    "    old_observation, action, reward, observation = sample_transition\n",
    "\n",
    "    targets = np.reshape(get_q(action_model, old_observation), ACTIONS_DIM)\n",
    "    targets[action] = reward\n",
    "    if observation is not None:\n",
    "      predictions = predict(target_model, observation)\n",
    "      new_action = np.argmax(predictions)\n",
    "      targets[action] += GAMMA * predictions[0, new_action]\n",
    "\n",
    "    batch_observations.append(old_observation)\n",
    "    batch_targets.append(targets)\n",
    "\n",
    "  train(action_model, batch_observations, batch_targets)\n",
    "\n",
    "def main():\n",
    "  steps_until_reset = TARGET_UPDATE_FREQ\n",
    "  random_action_probability = INITIAL_RANDOM_ACTION\n",
    "\n",
    "  # Initialize replay memory D to capacity N\n",
    "  replay = ReplayBuffer(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "  # Initialize action-value model with random weights\n",
    "  action_model = get_model()\n",
    "\n",
    "  # Initialize target model with same weights\n",
    "  #target_model = get_model()\n",
    "  #target_model.set_weights(action_model.get_weights())\n",
    "\n",
    "  env = gym.make('CartPole-v0')\n",
    "  env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1')\n",
    "\n",
    "  for episode in range(NUM_EPISODES):\n",
    "    observation = env.reset()\n",
    "\n",
    "    for iteration in range(MAX_ITERATIONS):\n",
    "      random_action_probability *= RANDOM_ACTION_DECAY\n",
    "      random_action_probability = max(random_action_probability, 0.1)\n",
    "      old_observation = observation\n",
    "\n",
    "      # if episode % 10 == 0:\n",
    "      #   env.render()\n",
    "\n",
    "      if np.random.random() < random_action_probability:\n",
    "        action = np.random.choice(range(ACTIONS_DIM))\n",
    "      else:\n",
    "        q_values = get_q(action_model, observation)\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "      observation, reward, done, info = env.step(action)\n",
    "\n",
    "      if done:\n",
    "        print 'Episode {}, iterations: {}'.format(\n",
    "          episode,\n",
    "          iteration\n",
    "        )\n",
    "\n",
    "        # print action_model.get_weights()\n",
    "        # print target_model.get_weights()\n",
    "\n",
    "        #print 'Game finished after {} iterations'.format(iteration)\n",
    "        reward = -200\n",
    "        replay.add(old_observation, action, reward, None)\n",
    "        break\n",
    "\n",
    "      replay.add(old_observation, action, reward, observation)\n",
    "\n",
    "      if replay.size() >= MINIBATCH_SIZE:\n",
    "        sample_transitions = replay.sample(MINIBATCH_SIZE)\n",
    "        update_action(action_model, action_model, sample_transitions)\n",
    "        steps_until_reset -= 1\n",
    "\n",
    "      # if steps_until_reset == 0:\n",
    "      #   target_model.set_weights(action_model.get_weights())\n",
    "      #   steps_until_reset = TARGET_UPDATE_FREQ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **mbalunovic** script with the adaptations to use ROSParams and use the new **TaskEnvironment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from gym import wrappers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "\n",
    "  def __init__(self, max_size):\n",
    "    self.max_size = max_size\n",
    "    self.transitions = deque()\n",
    "\n",
    "  def add(self, observation, action, reward, observation2):\n",
    "    if len(self.transitions) > self.max_size:\n",
    "      self.transitions.popleft()\n",
    "    self.transitions.append((observation, action, reward, observation2))\n",
    "\n",
    "  def sample(self, count):\n",
    "    return random.sample(self.transitions, count)\n",
    "\n",
    "  def size(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "def get_q(model, observation, state_size):\n",
    "  np_obs = np.reshape(observation, [-1, state_size])\n",
    "  return model.predict(np_obs)\n",
    "\n",
    "def train(model, observations, targets, actions_dim, state_size):\n",
    "\n",
    "  np_obs = np.reshape(observations, [-1, state_size])\n",
    "  np_targets = np.reshape(targets, [-1, actions_dim])\n",
    "\n",
    "  #model.fit(np_obs, np_targets, epochs=1, verbose=0)\n",
    "  model.fit(np_obs, np_targets, nb_epoch=1, verbose=0)\n",
    "\n",
    "def predict(model, observation, state_size):\n",
    "  np_obs = np.reshape(observation, [-1, state_size])\n",
    "  return model.predict(np_obs)\n",
    "\n",
    "def get_model(state_size, learning_rate):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(16, input_shape=(state_size, ), activation='relu'))\n",
    "  model.add(Dense(16, input_shape=(state_size,), activation='relu'))\n",
    "  model.add(Dense(2, activation='linear'))\n",
    "\n",
    "  model.compile(\n",
    "    optimizer=Adam(lr=learning_rate),\n",
    "    loss='mse',\n",
    "    metrics=[],\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "def update_action(action_model, target_model, sample_transitions, actions_dim, state_size, gamma):\n",
    "  random.shuffle(sample_transitions)\n",
    "  batch_observations = []\n",
    "  batch_targets = []\n",
    "\n",
    "  for sample_transition in sample_transitions:\n",
    "    old_observation, action, reward, observation = sample_transition\n",
    "\n",
    "    targets = np.reshape(get_q(action_model, old_observation, state_size), actions_dim)\n",
    "    targets[action] = reward\n",
    "    if observation is not None:\n",
    "      predictions = predict(target_model, observation, state_size)\n",
    "      new_action = np.argmax(predictions)\n",
    "      targets[action] += gamma * predictions[0, new_action]\n",
    "\n",
    "    batch_observations.append(old_observation)\n",
    "    batch_targets.append(targets)\n",
    "\n",
    "  train(action_model, batch_observations, batch_targets, actions_dim, state_size)\n",
    "\n",
    "def main():\n",
    "  \n",
    "  \n",
    "  state_size = rospy.get_param('/cartpole_v0/state_size')\n",
    "  action_size = rospy.get_param('/cartpole_v0/n_actions')\n",
    "  gamma = rospy.get_param('/cartpole_v0/gamma')\n",
    "  batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "  target_update_freq = rospy.get_param('/cartpole_v0/target_update_freq')\n",
    "  initial_random_action = rospy.get_param('/cartpole_v0/initial_random_action')\n",
    "  replay_memory_size = rospy.get_param('/cartpole_v0/replay_memory_size')\n",
    "  episodes_training = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "  max_iterations = rospy.get_param('/cartpole_v0/max_iterations')\n",
    "  epsilon_decay = rospy.get_param('/cartpole_v0/max_iterations')\n",
    "  learning_rate = rospy.get_param('/cartpole_v0/learning_rate')\n",
    "  done_episode_reward = rospy.get_param('/cartpole_v0/done_episode_reward')\n",
    "  \n",
    "  steps_until_reset = target_update_freq\n",
    "  random_action_probability = initial_random_action\n",
    "\n",
    "  # Initialize replay memory D to capacity N\n",
    "  replay = ReplayBuffer(replay_memory_size)\n",
    "\n",
    "  # Initialize action-value model with random weights\n",
    "  action_model = get_model(state_size, learning_rate)\n",
    "\n",
    "  env = gym.make('CartPoleStayUp-v0')\n",
    "  env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "  for episode in range(episodes_training):\n",
    "    observation = env.reset()\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "      random_action_probability *= epsilon_decay\n",
    "      random_action_probability = max(random_action_probability, 0.1)\n",
    "      old_observation = observation\n",
    "\n",
    "      # We dont support render in openai_ros\n",
    "      \"\"\"\n",
    "      if episode % 1 == 0:\n",
    "        env.render()\n",
    "      \"\"\"\n",
    "      \n",
    "      if np.random.random() < random_action_probability:\n",
    "        action = np.random.choice(range(action_size))\n",
    "      else:\n",
    "        q_values = get_q(action_model, observation, state_size)\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      \n",
    "\n",
    "      if done:\n",
    "        print 'Episode {}, iterations: {}'.format(\n",
    "          episode,\n",
    "          iteration\n",
    "        )\n",
    "\n",
    "        # print action_model.get_weights()\n",
    "        # print target_model.get_weights()\n",
    "\n",
    "        #print 'Game finished after {} iterations'.format(iteration)\n",
    "        reward = done_episode_reward\n",
    "        replay.add(old_observation, action, reward, None)\n",
    "        \n",
    "        break\n",
    "\n",
    "      replay.add(old_observation, action, reward, observation)\n",
    "\n",
    "      if replay.size() >= batch_size:\n",
    "        sample_transitions = replay.sample(batch_size)\n",
    "        update_action(action_model, action_model, sample_transitions, action_size, state_size, gamma)\n",
    "        steps_until_reset -= 1\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rospy.init_node('cartpole_mbalunovic_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The only main difference are these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_size = rospy.get_param('/cartpole_v0/state_size')\n",
    "  action_size = rospy.get_param('/cartpole_v0/n_actions')\n",
    "  gamma = rospy.get_param('/cartpole_v0/gamma')\n",
    "  batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "  target_update_freq = rospy.get_param('/cartpole_v0/target_update_freq')\n",
    "  initial_random_action = rospy.get_param('/cartpole_v0/initial_random_action')\n",
    "  replay_memory_size = rospy.get_param('/cartpole_v0/replay_memory_size')\n",
    "  episodes_training = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "  max_iterations = rospy.get_param('/cartpole_v0/max_iterations')\n",
    "  epsilon_decay = rospy.get_param('/cartpole_v0/max_iterations')\n",
    "  learning_rate = rospy.get_param('/cartpole_v0/learning_rate')\n",
    "  done_episode_reward = rospy.get_param('/cartpole_v0/done_episode_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also had to adapt the functions to use those parameters, but this is not realy related to OpenAI_ROS package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only **OpenAI_ROS** related changes are these two lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0') --> env = gym.make('CartPoleStayUp-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ruippeixotog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the **cartpole_ruippeixotog_algorithm**. The only difference is that because this one is divided into different files , the adaptations to use ROS might give a bit more work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Original files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_ruippeixotog_algorithm.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from gym_runner import GymRunner\n",
    "from q_learning_agent import QLearningAgent\n",
    "\n",
    "\n",
    "class CartPoleAgent(QLearningAgent):\n",
    "    def __init__(self):\n",
    "        QLearningAgent.__init__(self, 4, 2)\n",
    "        #super(CartPoleAgent, self).__init__(4, 2)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, activation='relu', input_dim=4))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(2))\n",
    "        model.compile(Adam(lr=0.001), 'mse')\n",
    "\n",
    "        # load the weights of the model if reusing previous training session\n",
    "        # model.load_weights(\"models/cartpole-v0.h5\")\n",
    "        return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rospy.init_node('cartpole_ruippeixotog_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    gym = GymRunner('CartPole-v0', 'gymresults/cartpole-v0')\n",
    "    agent = CartPoleAgent()\n",
    "\n",
    "    gym.train(agent, 1000)\n",
    "    gym.run(agent, 500)\n",
    "\n",
    "    agent.model.save_weights(\"models/cartpole-v0.h5\", overwrite=True)\n",
    "    gym.close_and_upload(os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a different version form the original, just due to some changes in systems and python compatibility. This is the version that works with python 2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole3D **cartpole_ruippeixotog_algorithm.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from gym_runner import GymRunner\n",
    "from q_learning_agent import QLearningAgent\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "class CartPoleAgent(QLearningAgent):\n",
    "    def __init__(self):\n",
    "        \n",
    "        state_size = rospy.get_param('/cartpole_v0/state_size')\n",
    "        action_size = rospy.get_param('/cartpole_v0/n_actions')\n",
    "        gamma = rospy.get_param('/cartpole_v0/gamma')\n",
    "        epsilon = rospy.get_param('/cartpole_v0/epsilon')\n",
    "        epsilon_decay = rospy.get_param('/cartpole_v0/epsilon_decay')\n",
    "        epsilon_min = rospy.get_param('/cartpole_v0/epsilon_min')\n",
    "        batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "        \n",
    "        \n",
    "        \n",
    "        QLearningAgent.__init__(self,\n",
    "                                state_size=state_size,\n",
    "                                action_size=action_size,\n",
    "                                gamma=gamma,\n",
    "                                epsilon=epsilon,\n",
    "                                epsilon_decay=epsilon_decay,\n",
    "                                epsilon_min=epsilon_min,\n",
    "                                batch_size=batch_size)\n",
    "        \n",
    "        #super(CartPoleAgent, self).__init__(4, 2)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, activation='relu', input_dim=4))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(2))\n",
    "        model.compile(Adam(lr=0.001), 'mse')\n",
    "\n",
    "        # load the weights of the model if reusing previous training session\n",
    "        # model.load_weights(\"models/cartpole-v0.h5\")\n",
    "        return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rospy.init_node('cartpole3D_ruippeixotog_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    episodes_training = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "    episodes_running = rospy.get_param('/cartpole_v0/episodes_running')\n",
    "    max_timesteps = rospy.get_param('/cartpole_v0/max_timesteps', 10000)\n",
    "    \n",
    "    gym = GymRunner('CartPoleStayUp-v0', 'gymresults/cartpole-v0', max_timesteps)\n",
    "    agent = CartPoleAgent()\n",
    "\n",
    "    gym.train(agent, episodes_training, do_train=True, do_render=False, publish_reward=False)\n",
    "    gym.run(agent, episodes_running, do_train=False, do_render=False, publish_reward=False)\n",
    "\n",
    "    agent.model.save_weights(\"models/cartpole-v0.h5\", overwrite=True)\n",
    "    #gym.close_and_upload(os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this file we had to also **Comment the Upload system**, just because its not suported in OpenAI_ROS for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the two auxiliary files that this algorithm has uses: **gym_runner.py**,  **q_learning_agent.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q_learning_agent.py** Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # hyperparameters\n",
    "        self.gamma = 0.95  # discount rate on future rewards\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995  # the decay of epsilon after each training batch\n",
    "        self.epsilon_min = 0.1  # the minimum exploration rate permissible\n",
    "        self.batch_size = 32  # maximum size of the batches sampled from memory\n",
    "\n",
    "        # agent state\n",
    "        self.model = self.build_model()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build_model(self):\n",
    "        return None\n",
    "\n",
    "    def select_action(self, state, do_train=True):\n",
    "        if do_train and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def record(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q_learning_agent.py** for Cartpole3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, batch_size=32):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # hyperparameters\n",
    "        self.gamma = gamma  # discount rate on future rewards\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # the decay of epsilon after each training batch\n",
    "        self.epsilon_min = epsilon_min  # the minimum exploration rate permissible\n",
    "        self.batch_size = batch_size  # maximum size of the batches sampled from memory\n",
    "\n",
    "        # agent state\n",
    "        self.model = self.build_model()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build_model(self):\n",
    "        return None\n",
    "\n",
    "    def select_action(self, state, do_train=True):\n",
    "        if do_train and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def record(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            #self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            # For changes in versions\n",
    "            self.model.fit(state, target_f, nb_epoch=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably see the only real changes were to add the capability of having parameters from outside the class be used ( for the ROSPARAMS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, state_size, action_size): --> def __init__(self, state_size, action_size, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, batch_size=32):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gym_runner.py** Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "class GymRunner:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=100000):\n",
    "        self.monitor_dir = monitor_dir\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.env = wrappers.Monitor(self.env, monitor_dir, force=True)\n",
    "\n",
    "    def calc_reward(self, state, action, gym_reward, next_state, done):\n",
    "        return gym_reward\n",
    "\n",
    "    def train(self, agent, num_episodes):\n",
    "        self.run(agent, num_episodes, do_train=True)\n",
    "\n",
    "    def run(self, agent, num_episodes, do_train=False):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "            total_reward = 0\n",
    "\n",
    "            for t in range(self.max_timesteps):\n",
    "                action = agent.select_action(state, do_train)\n",
    "\n",
    "                # execute the selected action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                reward = self.calc_reward(state, action, reward, next_state, done)\n",
    "\n",
    "                # record the results of the step\n",
    "                if do_train:\n",
    "                    agent.record(state, action, reward, next_state, done)\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # train the agent based on a sample of past experiences\n",
    "            if do_train:\n",
    "                agent.replay()\n",
    "\n",
    "            print(\"episode: {}/{} | score: {} | e: {:.3f}\".format(\n",
    "                episode + 1, num_episodes, total_reward, agent.epsilon))\n",
    "\n",
    "    def close_and_upload(self, api_key):\n",
    "        self.env.close()\n",
    "        gym.upload(self.monitor_dir, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gym_runner.py** Cartpole3D version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from openai_ros.msg import RLExperimentInfo\n",
    "import rospy\n",
    "\n",
    "class GymRunner:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=100000):\n",
    "        self.monitor_dir = monitor_dir\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.env = wrappers.Monitor(self.env, monitor_dir, force=True)\n",
    "        \n",
    "        self.pub_reward_obj = PublishRewardClass()\n",
    "\n",
    "    def calc_reward(self, state, action, gym_reward, next_state, done):\n",
    "        return gym_reward\n",
    "\n",
    "    def train(self, agent, num_episodes, do_train=True, do_render=True, publish_reward=True):\n",
    "        self.run(agent, num_episodes, do_train, do_render, publish_reward)\n",
    "\n",
    "    def run(self, agent, num_episodes, do_train=False, do_render=True, publish_reward=True):\n",
    "        \n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                \n",
    "                if do_render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                action = agent.select_action(state, do_train)\n",
    "\n",
    "                # execute the selected action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                reward = self.calc_reward(state, action, reward, next_state, done)\n",
    "                # Cumulate reward\n",
    "                self.pub_reward_obj.update_cumulated_reward(reward)\n",
    "\n",
    "                # record the results of the step\n",
    "                if do_train:\n",
    "                    agent.record(state, action, reward, next_state, done)\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    if publish_reward:\n",
    "                        self.pub_reward_obj._update_episode()\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "            # train the agent based on a sample of past experiences\n",
    "            if do_train:\n",
    "                agent.replay()\n",
    "\n",
    "            print(\"episode: {}/{} | score: {} | e: {:.3f}\".format(\n",
    "                episode + 1, num_episodes, total_reward, agent.epsilon))\n",
    "\n",
    "    def close_and_upload(self, api_key):\n",
    "        self.env.close()\n",
    "        gym.upload(self.monitor_dir, api_key=api_key)\n",
    "        \n",
    "        \n",
    "class PublishRewardClass(object):\n",
    "    def __init__(self):\n",
    "        # Set up ROS related variables\n",
    "        self.episode_num = 0\n",
    "        self.cumulated_episode_reward = 0\n",
    "        # Start Reward publishing\n",
    "        self.reward_pub = rospy.Publisher('/openai/reward', RLExperimentInfo, queue_size=1)\n",
    "        \n",
    "    def _publish_reward_topic(self, reward, episode_number=1):\n",
    "        \"\"\"\n",
    "        This function publishes the given reward in the reward topic for\n",
    "        easy access from ROS infrastructure.\n",
    "        :param reward:\n",
    "        :param episode_number:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        reward_msg = RLExperimentInfo()\n",
    "        reward_msg.episode_number = episode_number\n",
    "        reward_msg.episode_reward = reward\n",
    "        self.reward_pub.publish(reward_msg)\n",
    "        \n",
    "    def _update_episode(self):\n",
    "        \"\"\"\n",
    "        Publishes the cumulated reward of the episode and \n",
    "        increases the episode number by one.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._publish_reward_topic(\n",
    "                                    self.cumulated_episode_reward,\n",
    "                                    self.episode_num\n",
    "                                    )\n",
    "        self.episode_num += 1\n",
    "        self.cumulated_episode_reward = 0\n",
    "        \n",
    "    def update_cumulated_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Increase reward\n",
    "        \"\"\"\n",
    "        self.cumulated_episode_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only main changes are mainly again input parameters to have more flexible methods for the ROSPARAMS to be passed through. Note also the addition of the class **PublishRewardClass** which was only added in case you were to use a system outside the **OpenAI_ROS** package that didnt publish the rewards by default into our rewards topic **/openai/reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, agent, num_episodes): --> def train(self, agent, num_episodes, do_train=True, do_render=True, publish_reward=True):\n",
    "\n",
    "def run(self, agent, num_episodes, do_train=False): --> def run(self, agent, num_episodes, do_train=False, do_render=True, publish_reward=True):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an **if** statement for deactivating the render in the cases where we use Gazebo, which would be the case, because again, the **OpenAI_ROS** doesnt need to render anything, exept for videos, which is a work in progress during the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if do_render:\n",
    "    self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of this change in algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute the two new algorithms in **THE SAME TASKEnvironment** with very similar values in their implementations, you will get the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ruippeixotog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/cartpole_openai_ROSCON_ruipex_render_final.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole_openai_ROSCON_ruipex_standard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mbalunovic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/cartpole_malunovic_demo_ROSCON_render.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole_mbalunovic_standardvalues.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to launch these Demos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just launch the following after launching the main Cartpole3D simulation (remember in the Simulations Tab, **cartpole_description main.launch**):\n",
    "* mbalunovic:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " roslaunch cartpole_tests start_training_mbalunovic.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ruippeixotog:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roslaunch cartpole_tests start_training_ruippeixotog.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember to open the OpenAI-Rewards-Chart when it has published one or two episodes, to see their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Same Algorithm and Same Task, but we want to Test different Algorithm parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the algorithm that you think works best, how can you get the best posible paramter values? Well you have to iterate and test a few of them in the SAME exact conditions. Luckily, **OpenAI_ROS** in conjunction with **ROSDevelopement Studios** Gym Computers, makes that easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to test different Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to load different versions of **Yaml parameters files** when launching the same main **RL** script. Here you have three examples of different yaml files, changing the Gamma variable for the Qlearn RL of **ruippeixtog**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_ruippeixotog_params_gamma_high.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    state_size: 4 # number of elements in the state array from observations.\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    gamma: 0.9995 # future rewards value, 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.1 # minimum value that epsilon can have\n",
    "    batch_size: 32 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    episodes_running: 500\n",
    "    max_timesteps: 1000\n",
    "\n",
    "    #environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_ruippeixotog_params_gamma_mid.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    state_size: 4 # number of elements in the state array from observations.\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    gamma: 0.5 # future rewards value, 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.1 # minimum value that epsilon can have\n",
    "    batch_size: 32 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    episodes_running: 500\n",
    "    max_timesteps: 1000\n",
    "\n",
    "    #environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole_ruippeixotog_params_gamma_low.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartpole_v0: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    state_size: 4 # number of elements in the state array from observations.\n",
    "    n_actions: 2 # Number of actions used by algorithm and task\n",
    "    gamma: 0.1 # future rewards value, 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.1 # minimum value that epsilon can have\n",
    "    batch_size: 32 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    episodes_running: 500\n",
    "    max_timesteps: 1000\n",
    "\n",
    "    #environment variables\n",
    "    control_type: \"velocity\"\n",
    "    min_pole_angle: -0.7 #-23Â°\n",
    "    max_pole_angle: 0.7 #23Â°\n",
    "    max_base_velocity: 50\n",
    "    max_base_pose_x: 1.0\n",
    "    min_base_pose_x: -1.0\n",
    "    \n",
    "    # those parameters are very important. They are affecting the learning experience\n",
    "    # They indicate how fast the control can be\n",
    "    # If the running step is too large, then there will be a long time between 2 ctrl commans\n",
    "    # If the pos_step is too large, then the changes in position will be very abrupt\n",
    "    running_step: 0.04 # amount of time the control will be executed\n",
    "    pos_step: 1.0     # increment in position/velocity/effort, depends on the control for each command\n",
    "    init_pos: 0.0 # Position in which the base will start\n",
    "    wait_time: 0.1 # Time to wait in the reset phases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically have different Gamma values:\n",
    "* High: 0.9995\n",
    "* Mid: 0.5\n",
    "* Low: 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a generic launch file that will be used by the **Gym Multiple Computers System**. Notice that there is no loading of any yaml file. Thats because its loaded selected through the menu system for each conputer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_ruippeixotog_gymcomputers.launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"cartpole_tests\" name=\"cartpole3D_ruippeixotog_algorithm\" type=\"cartpole3D_ruippeixotog_algorithm.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select **Three Computers**, the launch file **start_training_ruippeixotog_gymcomputers.launch** and the yaml files for **each Computer**. And thats IT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of running Three simulations with the Same Algorithm, Same TAskEnvirnment, different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/cartpole_gamm3_render.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ruipex_3_gammas.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the same algorithm for different Robots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to use the algorithms that we know they worked in a simulation, in this case the **Cartpole3D**, with a new simulation that little has to do with it. In this case the Turtlebot2 simulation in a Maze. We want to use the same algorithm used by the Cartpole to learn how to StandUp, to learn how to navigate a simpla maze. They will use totally different sensors. Cartpole uses encoder data, while TurtleBot2 uses laser readings. And the Actions and diferent to, as well as the response time, the reseting system and the the overall problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartpole3D_n1try_algorithm_with_rosparams.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "\n",
    "# Inspired by https://keon.io/deep-q-learning/\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "class DQNRobotSolver():\n",
    "    def __init__(self, environment_name, n_observations, n_actions, n_episodes=1000, n_win_ticks=195, min_episodes= 100, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(environment_name)\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        \n",
    "        self.input_dim = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.min_episodes = min_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(24, input_dim=self.input_dim, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.input_dim])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        rate = rospy.Rate(30)\n",
    "        \n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            \n",
    "            init_state = self.env.reset()\n",
    "            \n",
    "            state = self.preprocess_state(init_state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # openai_ros doesnt support render for the moment\n",
    "                #self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= min_episodes:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials'.format(e, e - min_episodes))\n",
    "                return e - min_episodes\n",
    "            if e % 1 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last {} episodes was {} ticks.'.format(e, min_episodes ,mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "            \n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes'.format(e))\n",
    "        return e\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('cartpole_n1try_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    environment_name = 'CartPoleStayUp-v0'\n",
    "    \n",
    "    n_observations = rospy.get_param('/cartpole_v0/n_observations')\n",
    "    n_actions = rospy.get_param('/cartpole_v0/n_actions')\n",
    "    \n",
    "    n_episodes = rospy.get_param('/cartpole_v0/episodes_training')\n",
    "    n_win_ticks = rospy.get_param('/cartpole_v0/n_win_ticks')\n",
    "    min_episodes = rospy.get_param('/cartpole_v0/min_episodes')\n",
    "    max_env_steps = None\n",
    "    gamma =  rospy.get_param('/cartpole_v0/gamma')\n",
    "    epsilon = rospy.get_param('/cartpole_v0/epsilon')\n",
    "    epsilon_min = rospy.get_param('/cartpole_v0/epsilon_min')\n",
    "    epsilon_log_decay = rospy.get_param('/cartpole_v0/epsilon_decay')\n",
    "    alpha = rospy.get_param('/cartpole_v0/alpha')\n",
    "    alpha_decay = rospy.get_param('/cartpole_v0/alpha_decay')\n",
    "    batch_size = rospy.get_param('/cartpole_v0/batch_size')\n",
    "    monitor = rospy.get_param('/cartpole_v0/monitor')\n",
    "    quiet = rospy.get_param('/cartpole_v0/quiet')\n",
    "    \n",
    "    \n",
    "    agent = DQNRobotSolver(     environment_name,\n",
    "                                n_observations,\n",
    "                                n_actions,\n",
    "                                n_episodes,\n",
    "                                n_win_ticks,\n",
    "                                min_episodes,\n",
    "                                max_env_steps,\n",
    "                                gamma,\n",
    "                                epsilon,\n",
    "                                epsilon_min,\n",
    "                                epsilon_log_decay,\n",
    "                                alpha,\n",
    "                                alpha_decay,\n",
    "                                batch_size,\n",
    "                                monitor,\n",
    "                                quiet)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**turtlebot2_n1try_algorithm_with_rosparams.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import time\n",
    "# Inspired by https://keon.io/deep-q-learning/\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.turtlebot2 import turtlebot2_maze\n",
    "\n",
    "class DQNRobotSolver():\n",
    "    def __init__(self, environment_name, n_observations, n_actions, n_episodes=1000, n_win_ticks=195, min_episodes= 100, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(environment_name)\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/turtlebot2-1', force=True)\n",
    "        \n",
    "        self.input_dim = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.min_episodes = min_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(24, input_dim=self.input_dim, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.input_dim])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        rate = rospy.Rate(30)\n",
    "        \n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            \n",
    "            init_state = self.env.reset()\n",
    "            \n",
    "            state = self.preprocess_state(init_state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # openai_ros doesnt support render for the moment\n",
    "                #self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= min_episodes:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials'.format(e, e - min_episodes))\n",
    "                return e - min_episodes\n",
    "            if e % 1 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last {} episodes was {} ticks.'.format(e, min_episodes ,mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "            \n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes'.format(e))\n",
    "        return e\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('turtlebot2_n1try_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    environment_name = 'TurtleBot2Maze-v0'\n",
    "    \n",
    "    n_observations = rospy.get_param('/turtlebot2/n_observations')\n",
    "    n_actions = rospy.get_param('/turtlebot2/n_actions')\n",
    "    \n",
    "    n_episodes = rospy.get_param('/turtlebot2/episodes_training')\n",
    "    n_win_ticks = rospy.get_param('/turtlebot2/n_win_ticks')\n",
    "    min_episodes = rospy.get_param('/turtlebot2/min_episodes')\n",
    "    max_env_steps = None\n",
    "    gamma =  rospy.get_param('/turtlebot2/gamma')\n",
    "    epsilon = rospy.get_param('/turtlebot2/epsilon')\n",
    "    epsilon_min = rospy.get_param('/turtlebot2/epsilon_min')\n",
    "    epsilon_log_decay = rospy.get_param('/turtlebot2/epsilon_decay')\n",
    "    alpha = rospy.get_param('/turtlebot2/alpha')\n",
    "    alpha_decay = rospy.get_param('/turtlebot2/alpha_decay')\n",
    "    batch_size = rospy.get_param('/turtlebot2/batch_size')\n",
    "    monitor = rospy.get_param('/turtlebot2/monitor')\n",
    "    quiet = rospy.get_param('/turtlebot2/quiet')\n",
    "    \n",
    "    \n",
    "    agent = DQNRobotSolver(     environment_name,\n",
    "                                n_observations,\n",
    "                                n_actions,\n",
    "                                n_episodes,\n",
    "                                n_win_ticks,\n",
    "                                min_episodes,\n",
    "                                max_env_steps,\n",
    "                                gamma,\n",
    "                                epsilon,\n",
    "                                epsilon_min,\n",
    "                                epsilon_log_decay,\n",
    "                                alpha,\n",
    "                                alpha_decay,\n",
    "                                batch_size,\n",
    "                                monitor,\n",
    "                                quiet)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably see, there is not a lot of differences between the script for  **Cartpole3D** and for **Turtlebot2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only main differences are in two places:\n",
    "* The TaskEnvironment loaded: Cartpole script loads the **CartPoleStayUp-v0**, from **task_envs.cartpole_stay_up**. While the **Turtlebot2** loads **TurtleBot2Maze-v0** from **task_envs.turtlebot2**.\n",
    "* The Values of the learning parameters change: We are loading when we launch different **yaml** files, just because each task has different **observations dimensions** and **actions dimensions**. Cartpole has only **two** actions ( speed left, speed right ), while Turtlebot2 has **three** ( Go Left, Go Right and Go Forwards). And the same with observations. Cartpole only has **four** observations ( angular position/vel of pole, linear position/vel of base ) while the Turtlebot2 has a variable number of observations depending on how many laser readings you want to consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai_ros.task_envs.cartpole_stay_up import stay_up\n",
    "\n",
    "environment_name = 'CartPoleStayUp-v0'\n",
    "\n",
    "\n",
    "from openai_ros.task_envs.turtlebot2 import turtlebot2_maze\n",
    "\n",
    "environment_name = 'TurtleBot2Maze-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it. We now can create the launch file for the turtlebot2 and its config file. The config file basic values can be retrieved form the git of openai_ros_examples : https://bitbucket.org/theconstructcore/openai_examples_projects/src/master/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_n1try.launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find turtlebot2_tests)/config/turtlebot2_n1try_params.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"turtlebot2_tests\" name=\"cartpole3D_n1try_algorithm\" type=\"turtlebot2_n1try_algorithm_with_rosparams.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**turtlebot2_n1try_params.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turtlebot2: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    \n",
    "    alpha: 0.01 # Learning Rate\n",
    "    alpha_decay: 0.01\n",
    "    gamma: 1.0 # future rewards value 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.01 # minimum value that epsilon can have\n",
    "    batch_size: 64 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 1000\n",
    "    n_win_ticks: 250 # If the mean of rewards is bigger than this and have passed min_episodes, the task is considered finished\n",
    "    min_episodes: 100\n",
    "    #max_env_steps: None\n",
    "    monitor: True\n",
    "    quiet: False\n",
    "    \n",
    "\n",
    "    # Turtlebot Realated parameters\n",
    "    number_decimals_precision_obs: 4\n",
    "    speed_step: 1.0 # Time to wait in the reset phases\n",
    "\n",
    "    linear_forward_speed: 0.3 # Spwwed for ging fowards\n",
    "    linear_turn_speed: 0.1 # Lienare speed when turning\n",
    "    angular_speed: 0.3 # Angular speed when turning Left or Right\n",
    "    init_linear_forward_speed: 0.0 # Initial linear speed in which we start each episode\n",
    "    init_linear_turn_speed: 0.0 # Initial angular speed in shich we start each episode\n",
    "    \n",
    "    n_observations: 8 # Number of lasers to consider in the observations\n",
    "    n_actions: 3 # Number of actions used by algorithm and task\n",
    "    #new_ranges: 50 # How many laser readings we jump in each observation reading, the bigger the less laser resolution\n",
    "    min_range: 0.2 # Minimum meters below wich we consider we have crashed\n",
    "    max_laser_value: 6 # Value considered Ok, no wall\n",
    "    min_laser_value: 0 # Value considered there is an obstacle or crashed\n",
    "    \n",
    "    desired_pose:\n",
    "      x: 5.0\n",
    "      y: 0.0\n",
    "      z: 0.0\n",
    "    \n",
    "    forwards_reward: 1 # Points Given to go forwards\n",
    "    turn_reward: 1 # Points Given to turn as action\n",
    "    end_episode_points: 200 # Points given when ending an episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it, you can now execute the demo and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to execute the demo of Turtlebot2 with n1try algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the simulation for turtlebot in the maze- For that you have to launch through the **Simulations drop down menu**:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cartpole_description main.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when its ready, launch the training script launch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/catkin_ws\n",
    "source devel/setup.bash\n",
    "roslaunch turtlebot2_tests start_training_n1try.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get results similar to the fllowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/turtlebot2_n1try_demo_ROSCON_render.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Turtlebot2 after 400 episodes running it considers the task solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/turtlebot2_n1try_SOLVEDafter400episodes.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Save your Learned Models and retrieve it afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As extra, we here show you a moded version of the **n1try algorithm** for the turtlebot2-maze **TaskEnvironment** that is optimised, improved and moded to save and load the model learned to be retrieved and execute a run using only whta it has leraned in the training fase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**turtlebot2_n1try_algorithm_with_rosparams_savemodel.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import time\n",
    "# Inspired by https://keon.io/deep-q-learning/\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "import rospkg\n",
    "import os\n",
    "\n",
    "# import our training environment\n",
    "from openai_ros.task_envs.turtlebot2 import turtlebot2_maze\n",
    "\n",
    "class DQNRobotSolver():\n",
    "    def __init__(self, environment_name, n_observations, n_actions, n_win_ticks=195, min_episodes= 100, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(environment_name)\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/turtlebot2-1', force=True)\n",
    "        \n",
    "        self.input_dim = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.min_episodes = min_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(24, input_dim=self.input_dim, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon, do_train, iteration=0):\n",
    "        \n",
    "        if do_train and (np.random.random() <= epsilon):\n",
    "            # We return a random sample form the available action space\n",
    "            rospy.logfatal(\">>>>>Chosen Random ACTION\")\n",
    "            action_chosen = self.env.action_space.sample()\n",
    "             \n",
    "        else:\n",
    "            # We return the best known prediction based on the state\n",
    "            action_chosen = np.argmax(self.model.predict(state))\n",
    "        \n",
    "        if do_train:\n",
    "            rospy.logfatal(\"LEARNING A=\"+str(action_chosen)+\",E=\"+str(round(epsilon, 3))+\",I=\"+str(iteration))\n",
    "        else:\n",
    "            rospy.logfatal(\"RUNNING A=\"+str(action_chosen)+\",E=\"+str(round(epsilon, 3))+\",I=\"+str(iteration))\n",
    "        \n",
    "        return action_chosen\n",
    "        \n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        new_epsilon = max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "        #new_epsilon = self.epsilon\n",
    "        return new_epsilon\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.input_dim])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self, num_episodes, do_train=False):\n",
    "        \n",
    "        rate = rospy.Rate(30)\n",
    "        \n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(num_episodes):\n",
    "            \n",
    "            init_state = self.env.reset()\n",
    "            \n",
    "            state = self.preprocess_state(init_state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # openai_ros doesnt support render for the moment\n",
    "                #self.env.render()\n",
    "                action = self.choose_action(state, self.get_epsilon(e), do_train, i)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                \n",
    "                if do_train:\n",
    "                    # If we are training we want to remember what I did and process it.\n",
    "                    self.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= min_episodes:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials'.format(e, e - min_episodes))\n",
    "                return e - min_episodes\n",
    "            if e % 1 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last {} episodes was {} ticks.'.format(e, min_episodes ,mean_score))\n",
    "\n",
    "            if do_train:\n",
    "                self.replay(self.batch_size)\n",
    "            \n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes'.format(e))\n",
    "        return e\n",
    "        \n",
    "    def save(self, model_name, models_dir_path=\"/tmp\"):\n",
    "        \"\"\"\n",
    "        We save the current model\n",
    "        \"\"\"\n",
    "        \n",
    "        model_name_yaml_format = model_name+\".yaml\"\n",
    "        model_name_HDF5_format = model_name+\".h5\"\n",
    "        \n",
    "        model_name_yaml_format_path = os.path.join(models_dir_path,model_name_yaml_format)\n",
    "        model_name_HDF5_format_path = os.path.join(models_dir_path,model_name_HDF5_format)\n",
    "        \n",
    "        # serialize model to YAML\n",
    "        model_yaml = self.model.to_yaml()\n",
    "        \n",
    "        with open(model_name_yaml_format_path, \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "        # serialize weights to HDF5: http://www.h5py.org/\n",
    "        self.model.save_weights(model_name_HDF5_format_path)\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "    def load(self, model_name, models_dir_path=\"/tmp\"):\n",
    "        \"\"\"\n",
    "        Loads a previously saved model\n",
    "        \"\"\"\n",
    "        \n",
    "        model_name_yaml_format = model_name+\".yaml\"\n",
    "        model_name_HDF5_format = model_name+\".h5\"\n",
    "        \n",
    "        model_name_yaml_format_path = os.path.join(models_dir_path,model_name_yaml_format)\n",
    "        model_name_HDF5_format_path = os.path.join(models_dir_path,model_name_HDF5_format)\n",
    "        \n",
    "        # load yaml and create model\n",
    "        yaml_file = open(model_name_yaml_format_path, 'r')\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        yaml_file.close()\n",
    "        self.model = model_from_yaml(loaded_model_yaml)\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(model_name_HDF5_format_path)\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('turtlebot2_n1try_algorithm', anonymous=True, log_level=rospy.FATAL)\n",
    "    \n",
    "    rospackage_name = \"turtlebot2_tests\"\n",
    "    model_name = \"turtlebot2_n1try\"\n",
    "    \n",
    "    environment_name = 'TurtleBot2Maze-v0'\n",
    "    \n",
    "    n_observations = rospy.get_param('/turtlebot2/n_observations')\n",
    "    n_actions = rospy.get_param('/turtlebot2/n_actions')\n",
    "    \n",
    "    n_episodes_training = rospy.get_param('/turtlebot2/episodes_training')\n",
    "    n_episodes_running = rospy.get_param('/turtlebot2/episodes_running')\n",
    "    n_win_ticks = rospy.get_param('/turtlebot2/n_win_ticks')\n",
    "    min_episodes = rospy.get_param('/turtlebot2/min_episodes')\n",
    "    max_env_steps = None\n",
    "    gamma =  rospy.get_param('/turtlebot2/gamma')\n",
    "    epsilon = rospy.get_param('/turtlebot2/epsilon')\n",
    "    epsilon_min = rospy.get_param('/turtlebot2/epsilon_min')\n",
    "    epsilon_log_decay = rospy.get_param('/turtlebot2/epsilon_decay')\n",
    "    alpha = rospy.get_param('/turtlebot2/alpha')\n",
    "    alpha_decay = rospy.get_param('/turtlebot2/alpha_decay')\n",
    "    batch_size = rospy.get_param('/turtlebot2/batch_size')\n",
    "    monitor = rospy.get_param('/turtlebot2/monitor')\n",
    "    quiet = rospy.get_param('/turtlebot2/quiet')\n",
    "    \n",
    "    \n",
    "    agent = DQNRobotSolver(     environment_name,\n",
    "                                n_observations,\n",
    "                                n_actions,\n",
    "                                n_win_ticks,\n",
    "                                min_episodes,\n",
    "                                max_env_steps,\n",
    "                                gamma,\n",
    "                                epsilon,\n",
    "                                epsilon_min,\n",
    "                                epsilon_log_decay,\n",
    "                                alpha,\n",
    "                                alpha_decay,\n",
    "                                batch_size,\n",
    "                                monitor,\n",
    "                                quiet)\n",
    "    agent.run(num_episodes=n_episodes_training, do_train=True)\n",
    "    \n",
    "    \n",
    "    rospack = rospkg.RosPack()\n",
    "    pkg_path = rospack.get_path(rospackage_name)\n",
    "    outdir = pkg_path + '/models'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "        rospy.logfatal(\"Created folder=\"+str(outdir))\n",
    "    \n",
    "    agent.save(model_name, outdir)\n",
    "    agent.load(model_name, outdir)\n",
    "    \n",
    "    agent.run(num_episodes=n_episodes_running, do_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the newest elements like the save, load and run fetures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rospackage_name = \"turtlebot2_tests\"\n",
    "model_name = \"turtlebot2_n1try\"\n",
    "\n",
    "rospack = rospkg.RosPack()\n",
    "pkg_path = rospack.get_path(rospackage_name)\n",
    "outdir = pkg_path + '/models'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    rospy.logfatal(\"Created folder=\"+str(outdir))\n",
    "\n",
    "agent.save(model_name, outdir)\n",
    "agent.load(model_name, outdir)\n",
    "\n",
    "agent.run(num_episodes=n_episodes_running, do_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the models are saved in two parts:\n",
    "* The Model Structure: Its saved in a yaml file.\n",
    "* The Model Weights: Are saved in a format called **HDF5** http://www.h5py.org/. Used for N dimensional matrix data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**turtlebot2_n1try_params_save_and_load.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turtlebot2: #namespace\n",
    "\n",
    "    #qlearn parameters\n",
    "    \n",
    "    alpha: 0.01 # Learning Rate\n",
    "    alpha_decay: 0.01\n",
    "    gamma: 1.0 # future rewards value 0 none 1 a lot\n",
    "    epsilon: 1.0 # exploration, 0 none 1 a lot\n",
    "    epsilon_decay: 0.995 # how we reduse the exploration\n",
    "    epsilon_min: 0.01 # minimum value that epsilon can have\n",
    "    batch_size: 64 # maximum size of the batches sampled from memory\n",
    "    episodes_training: 40\n",
    "    episodes_running: 10\n",
    "    n_win_ticks: 80 # If the mean of rewards is bigger than this and have passed min_episodes, the task is considered finished\n",
    "    min_episodes: 10\n",
    "    #max_env_steps: None\n",
    "    monitor: True\n",
    "    quiet: False\n",
    "    \n",
    "\n",
    "    # Turtlebot Realated parameters\n",
    "    number_decimals_precision_obs: 4\n",
    "    speed_step: 1.0 # Time to wait in the reset phases\n",
    "\n",
    "    linear_forward_speed: 0.3 # Speed for going fowards\n",
    "    linear_turn_speed: 0.2 # Lienare speed when turning\n",
    "    angular_speed: 0.3 # Angular speed when turning Left or Right\n",
    "    init_linear_forward_speed: 0.0 # Initial linear speed in which we start each episode\n",
    "    init_linear_turn_speed: 0.0 # Initial angular speed in shich we start each episode\n",
    "    \n",
    "    n_observations: 8 # Number of lasers to consider in the observations\n",
    "    n_actions: 3 # Number of actions used by algorithm and task\n",
    "    #new_ranges: 50 # How many laser readings we jump in each observation reading, the bigger the less laser resolution\n",
    "    min_range: 0.2 # Minimum meters below wich we consider we have crashed\n",
    "    max_laser_value: 6 # Value considered Ok, no wall\n",
    "    min_laser_value: 0 # Value considered there is an obstacle or crashed\n",
    "    \n",
    "    desired_pose:\n",
    "      x: 5.0\n",
    "      y: 0.0\n",
    "      z: 0.0\n",
    "    \n",
    "    forwards_reward: 1 # Points Given to go forwards\n",
    "    turn_reward: 1 # Points Given to turn as action\n",
    "    end_episode_points: 200 # Points given when ending an episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_training_n1try_save_and_load.launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <rosparam command=\"load\" file=\"$(find turtlebot2_tests)/config/turtlebot2_n1try_params_save_and_load.yaml\" />\n",
    "    <!-- Launch the training system -->\n",
    "    <node pkg=\"turtlebot2_tests\" name=\"cartpole3D_n1try_algorithm\" type=\"turtlebot2_n1try_algorithm_with_rosparams_savemodel.py\" output=\"screen\"/>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To launch this demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the simulation for turtlebot in the maze- For that you have to launch through the **Simulations drop down menu**:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cartpole_description main.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when its ready, launch the training script launch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/catkin_ws\n",
    "source devel/setup.bash\n",
    "roslaunch turtlebot2_tests start_training_n1try_save_and_load.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get results similar to the fllowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "      <source src=\"videos/turtlebot_movementdelta0-2_40episodes_10running_10xsimspeed.mp4\" type=\"video/mp4\">\n",
       "</video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"videos/turtlebot_movementdelta0-2_40episodes_10running_10xsimspeed.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Turtlebot2 after 40 episodes learning and 10 running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/turtlebot_movementdelta0-2_40episodes_10running_10xsimspeed.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lathough you can easily interchange one Task to another with **OpenAI_ROS** , each simulation has its own dynamics. For example, Cartpole3D has a need for a very fast and subtile actions. One tenth of a second late can make the pole fall. Therefore its important that the reaction times and processing times are fast, and that all the minute posibilities are explored. Therefore the observations space has to be precise but limited. Otherwise it would take ages to explore all the possible states ( with a need of a very slow degrading epsilon) . If the epsilon is to fast in degrading and you have too much states to explore it can result in the system learning aonly a limited number of paths.\n",
    "\n",
    "In turtlebot is the same issue. Because the posiblities of states are bigger ( we have much more laser readings than the cartpole to define the state ). Therefore a fast degrading epsilon results in the system only learning a simple path, and as the episodes pass, it becaisme impossible that a ranodm action can affect the state of the robot if the actions are take very frequently, resulting in small changes in the direction. In this case actions work better if they are taken les frequently and their effects are more pronounced, allowing the random actions have bugger effects and exploring more space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
